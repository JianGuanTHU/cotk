r"""
``cotk.metrics`` provides classes and functions evaluating results of models.
It provides a fair metric for every model.
"""
from itertools import chain
import numpy as np
from .._utils.unordered_hash import UnorderedSha256
from .._utils.imports import DummyObject
from .._utils.metaclass import LoadClassInterface, DocStringInheritor
try:
	import torch
except ImportError as err:
	torch = DummyObject(err)
	torch.Tensor = DummyObject(err)

class MetricBase(LoadClassInterface, metaclass=DocStringInheritor):
	'''Base class for metrics.
	'''

	DATALOADER_ARGUMENTS = \
		r"""dataloader (:class:`.dataloader.LanguageProcessingBase`): A language generation dataloader."""
	REFERENCE_ALLVOCABS_KEY_ARGUMENTS = \
		r"""reference_allvocabs_key (str):
			The key of reference sentences. Default: ``ref_allvocabs``."""
	FORWARD_REFERENCE_ALLVOCABS_ARGUMENTS = \
				r"""* **data[reference_allvocabs_key]** (list or :class:`numpy.ndarray`):
				  A 2-d jagged or padded array of int. Reference sentences with
				  :ref:`allvocabs <vocab_ref>` in index form.
				  Contains start token (eg: ``<go>``) and end token (eg: ``<eos>``).
				  Size: ``[batch_size, ~ref_sentence_length]``,
				  where "~" means different sizes in this dimension is allowed."""
	FORWARD_REFERENCE_ALLVOCABS_ARGUMENTS_WITH_TORCH = \
		FORWARD_REFERENCE_ALLVOCABS_ARGUMENTS.replace("list or :class:`numpy.ndarray`", \
			"list or :class:`numpy.ndarray` or :class:`torch.Tensor`")
	FORWARD_POST_ALLVOCABS_ARGUMENTS = \
		FORWARD_REFERENCE_ALLVOCABS_ARGUMENTS.replace("reference_allvocabs_key", \
			"post_allvocabs_key")
	FORWARD_RESP_ALLVOCABS_ARGUMENTS = \
		FORWARD_REFERENCE_ALLVOCABS_ARGUMENTS.replace("reference_allvocabs_key", \
			"resp_allvocabs_key")

	LABEL_KEY_ARGUMENTS = \
		r"""label_key (str):
			The key of reference sentence labels. Default: ``label``."""
	LABEL_ARGUMENTS = r"""* **data[label_key]** (list or :class:`numpy.ndarray`):
				  A 1-d array of int.
				  Size: ``[batch_size]``,
				  each element refers to label of one sample"""

	PREDICTION_KEY_ARGUMENTS = \
		r"""prediction_key (str):
			The key of reference sentence predictions. Default: ``prediction``."""
	PREDICTION_ARGUMENTS = r"""* **data[prediction_key]** (list or :class:`numpy.ndarray`):
				  A 1-d array of int.
				  Size: ``[batch_size]``,
				  each element refers to prediction for one sample"""

	MULTI_TURN_REFERENCE_ALLVOCABS_KEY_ARGUMENTS = \
		r"""multi_turn_reference_allvocabs_key (str):
			The key of reference sentences. Default: ``multi_turn_ref_allvocabs``."""
	FORWARD_MULTI_TURN_REFERENCE_ALLVOCABS_ARGUMENTS = \
				r"""* **data[multi_turn_reference_allvocabs_key]** (list or :class:`numpy.ndarray`):
				  A 3-d jagged or padded array of int. Multi-turn reference sentences with
				  :ref:`all vocabs <vocab_ref>`. Contains start token (eg: ``<go>``) and
				  end token (eg: ``<eos>``). Size: ``[batch_size, ~turn_length, ~sentence_length]``,
				  where "~" means different sizes in this dimension is allowed."""
	FORWARD_MULTI_TURN_REFERENCE_ALLVOCABS_ARGUMENTS_WITH_TORCH = \
		FORWARD_MULTI_TURN_REFERENCE_ALLVOCABS_ARGUMENTS.replace("list or :class:`numpy.ndarray`", \
			"list or :class:`numpy.ndarray` or :class:`torch.Tensor`")
	FORWARD_MULTI_TURN_CONTEXT_ALLVOCABS_ARGUMENTS = \
		FORWARD_MULTI_TURN_REFERENCE_ALLVOCABS_ARGUMENTS.replace("reference_allvocabs_key", \
			"context_allvocabs_key")

	REFERENCE_LEN_KEY_ARGUMENTS = \
		r"""reference_len_key (str):
			The key of lengths of reference sentences.
			Default: ``ref_length``."""
	FORWARD_REFERENCE_LEN_ARGUMENTS = \
				r"""* **data[reference_len_key]** (list or :class:`numpy.ndarray`):
				  Length of reference sentences. Contains start token (eg:``<go>``)
				  and end token (eg:``<eos>``). Size: ``[batch_size]``."""

	MULTI_TURN_REFERENCE_LEN_KEY_ARGUMENTS = \
		r"""multi_turn_reference_len_key (str):
			The key of lengths of reference sentences.
			Default: ``multi_turn_ref_length``."""
	FORWARD_MULTI_TURN_REFERENCE_LEN_ARGUMENTS = \
				r"""* **data[multi_turn_reference_len_key]** (list or :class:`numpy.ndarray`):
				  A 2-d jagged or padded array of int. **If padded, redundant position must be set to** ``0``.
				  Length of multi-turn reference sentences. Contains start token (eg:``<go>``)
				  and end token (eg:``<eos>``). Size: ``[batch_size, ~turn_length]``,
				  where "~" means different sizes in this dimension is allowed."""

	GEN_KEY_ARGUMENTS = \
		r"""gen_key (str):
			The key of generated sentences. Default: ``gen``."""
	FORWARD_GEN_ARGUMENTS = \
				r"""* **data[gen_key]** (list or :class:`numpy.ndarray`):
				  A 2-d jagged or padded array of int.
				  Sentences generated by model. Contains end token (eg: ``<eos>``),
				  but without start token (eg: ``<go>``).
				  Size: ``[batch_size, ~gen_sentence_length]``,
				  where "~" means different sizes in this dimension is allowed."""

	MULTI_TURN_GEN_KEY_ARGUMENTS = \
		r"""multi_turn_gen_key (str):
			The key of generated sentences. Default: ``multi_turn_gen``."""
	FORWARD_MULTI_TURN_GEN_ARGUMENTS = \
				r"""* **data[gen_key]** (list or :class:`numpy.ndarray`):
				  A 3-d jagged or padded array of int. Sentences generated by model.
				  Contains end token (eg: ``<eos>``), but without start token (eg: ``<go>``).
				  Size: ``[batch_size, ~max_turn_length, ~gen_sentence_length]``,
				  where "~" means different sizes in this dimension is allowed."""

	MULTI_TURN_LENGTH_KEY_ARGUMENTS = \
		r"""turn_length (str):
			The key of length of turns. Default: ``turn_length``."""
	FORWARD_MULTI_TURN_LENGTH_ARGUMENTS = \
				r"""* **data[turn_len_key]** (list or :class:`numpy.ndarray`):
				  Length of turns in each sample.
				  Size: ``[batch_size]``."""

	CPU_COUNT_ARGUMENTS = \
		r"""cpu_count (int): Number of used cpu for multiprocessing. Multiprocessing will **NOT** be used
			when ``cpu_count`` is set to ``1`` or the dataset is small. Default: if ``None``,
			the environment variable ``CPU_COUNT`` will be used	when it is set,
			or all available cpu will be used otherwise."""

	def __init__(self):
		self.unordered_hash = UnorderedSha256()
		self.closed = False

	def _hash_relevant_data(self, data_list):
		'''Invoked by :meth:`.forward` or :meth:`.close` to hash relevant data when computing a metric.

		Arguments:
			data_list (list): relevant data organized as list.
		'''
		for item in data_list:
			self.unordered_hash.update_data(repr(item).encode())

	def _hashvalue(self):
		'''Invoked by :meth:`.close` to return the recorded hash value.
		'''
		return self.unordered_hash.digest()

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict contains the data that metrics need.
		'''
		if self.closed:
			raise ValueError("The metric has been closed.")
		if not isinstance(data, dict):
			raise TypeError("Data must be a dict.")

	def close(self):
		'''
		Close the metric and return results. Once the metric is closed,
		any operation on the metric (e.g. forward or another close) will raise a ValueError.

		Returns:
			(dict) which contains results.
		'''
		if not self.closed:
			self.closed = True
			return {}
		else:
			raise RuntimeError("The metric has been closed.")

class _PrecisionRecallMetric(MetricBase):
	r"""Base class for precision recall metrics. This is an abstract class.

	Arguments:
		{ARGUMENTS}
	Attributes:
		res_prefix (str): Prefix added to the front of each key
					in the result dict of `close`.
	"""

	ARGUMENTS = r"""
		{MetricBase.DATALOADER_ARGUMENTS}
		generated_num_per_context (int): The number of sentences generated per context.
		candidate_allvocabs_key (str): The key of reference sentences. Default: ``candidate_allvocabs``.
		multiple_gen_key (str):
			The key of multiple generated sentences. Default: ``multiple_gen``."""


	def __init__(self, dataloader, \
				 generated_num_per_context, \
				 candidate_allvocabs_key='candidate_allvocabs', \
				 multiple_gen_key='multiple_gen'):
		super().__init__()
		self.dataloader = dataloader
		self.candidate_allvocabs_key = candidate_allvocabs_key
		self.multiple_gen_key = multiple_gen_key
		self.generated_num_per_context = generated_num_per_context
		self.prec_list = []
		self.rec_list = []
		self.res_prefix = ""

	def _score(self, gen, reference):
		r'''This function is called by :func:`forward`.

		Arguments:
			gen (list): list of generated word ids.
			reference (list): list of word ids of a reference.

		Returns:
			int: score \in [0, 1].
		'''
		raise NotImplementedError( \
			"This function should be implemented by subclasses.")

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict at least contains the following keys:

				* **data[candidate_allvocabs_key]** (list or :class:`numpy.ndarray`):
				  A 3-d jagged list of index. Multiple reference sentences for a single context.
				  Does not contain start token (eg: ``<go>``) and end token (eg: ``<eos>``).
				  Size: ``[batch_size, ~sentence_num, ~word_num]``, where "~" means different sizes
				  in this dimension is allowed.
				* **data[multiple_gen_key]** (list or :class:`numpy.ndarray`):
				  A 3-d jagged or padded array.
				  Sentences generated by model. Contains end token (eg: ``<eos>``),
				  but without start token (eg: ``<go>``).
				  Size: ``[batch_size, generated_num_per_context, ~gen_sentence_length]``,
				  where "~" means different sizes in this dimension is allowed.

				Here is an example for data:
					>>> # all_vocab_list = ["<pad>", "<unk>", "<go>", "<eos>", "I", "have",
					>>> #   "been", "to", "China"]
					>>> data = {
					... 	candidate_allvocabs_key: [[[4], [5,6]], [[4,5,6]]]
					...		multiple_gen_key: [[[5,6,3]], [[4,5,7,3], [8,3]]]
					... }

		'''
		super().forward(data)
		candidate_allvocabs = data[self.candidate_allvocabs_key]
		multiple_gen = data[self.multiple_gen_key]

		if not isinstance(candidate_allvocabs, (np.ndarray, list)):
			raise TypeError("Unknown type for candidate_allvocabs.")
		if not isinstance(multiple_gen, (np.ndarray, list)):
			raise TypeError("Unknown type for multiple_gen")

		references = [[self.dataloader.trim_index(cand[1:]) for cand in inst] \
					  for inst in candidate_allvocabs]
		gens = [[self.dataloader.trim_index(cand) for cand in inst] \
					  for inst in multiple_gen]

		if len(references) != len(gens):
			raise ValueError("Batch num is not matched.")

		for line in gens:
			if len(line) != self.generated_num_per_context:
				raise ValueError(\
					"Number of geneated sentences per context does not equal to\
					the specified `generated_num_per_context`")

		self._hash_relevant_data(list(chain(*references)))
		for reference, gen in zip(references, gens):
			# pylint: disable=no-member
			matrix = np.zeros((len(reference), len(gen)), dtype=np.float32)
			for i, single_ref in enumerate(reference):
				for j, single_gen in enumerate(gen):
					matrix[i][j] = self._score(single_gen, single_ref)
			self.prec_list.append(float(np.sum(np.max(matrix, 0))) / len(gen))
			self.rec_list.append(float(np.sum(np.max(matrix, 1))) / len(reference))

	def close(self):
		'''
		Returns:
			(dict) returns a dict which contains

			* ``res_prefix`` **precision**: average precision.
			* ``res_prefix`` **recall**: average recall.
			* ``res_prefix`` **hashvalue**: hash value for precision & recall metric, same hash value stands
			  for same evaluation settings.

		'''
		if (not self.prec_list) or (not self.rec_list):
			raise RuntimeError("The metric has not been forwarded data correctly.")
		res = super().close()
		res.update({'{} precision'.format(self.res_prefix): np.average(self.prec_list), \
				'{} recall'.format(self.res_prefix): np.average(self.rec_list), \
				'{} hashvalue'.format(self.res_prefix): self._hashvalue()})
		return res

class MetricChain(MetricBase):
	'''A metric-like class for stacked metric. You can use this class
	making multiples metric combination like one.

	Examples:
		>>> metric = MetricChain()
		>>> metric.add_metric(BleuCorpusMetric())
		>>> metric.add_metric(SingleDialogRecorder(dataloader))

	Todo: Give more examples to combining forward and close
	'''
	def __init__(self):
		super().__init__()
		self.metric_list = []

	def add_metric(self, metric):
		'''Add metric for processing.

		Arguments:
			metric (MetricBase): a metric class.
		'''
		if not isinstance(metric, MetricBase):
			raise TypeError("Metric must be a subclass of MetricBase")
		self.metric_list.append(metric)

	def forward(self, data):
		'''Processing a batch of data.

		Arguments:
			data (dict): A dict at least contains keys which all the
				metric components need.
		'''
		super().forward(data)
		for metric in self.metric_list:
			metric.forward(data)

	def close(self):
		r'''
		Returns:
			(dict): A dict which contains the items which all the
			metric components returned.
		'''
		res = super().close()
		for metric in self.metric_list:
			res.update(metric.close())
		return res
